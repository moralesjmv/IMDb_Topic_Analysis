{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "#!pip install ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie_summaries_bottom_250.txt', 'r') as myfile:\n",
    "    summary=myfile.readlines()\n",
    "with open('movie_titles_bottom_250.txt', 'r') as myfile:\n",
    "    title= myfile.readlines()\n",
    "title = [x.strip() for x in title] \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip any proper names from a text...unfortunately right now this is yanking the first word from a sentence too.\n",
    "import string\n",
    "def strip_proppers(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.islower()]\n",
    "    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#strip any proper nouns (NNP) or plural proper nouns (NNPS) from a text\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def strip_proppers_POS(text):\n",
    "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
    "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "    return non_propernouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 422 ms\n",
      "Wall time: 552 ms\n",
      "Wall time: 45.6 ms\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities \n",
    "\n",
    "#remove proper names\n",
    "%time preprocess = [strip_proppers(doc) for doc in summary]\n",
    "#tokenize\n",
    "%time tokenized_text = [tokenize_and_stem(text) for text in preprocess]\n",
    "#remove stop words\n",
    "%time texts = [[word for word in text if word not in stopwords] for text in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "#convert the dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.3 s\n"
     ]
    }
   ],
   "source": [
    "%time lda = models.LdaModel(corpus, num_topics=5, id2word=dictionary, update_every=5, chunksize=10000, passes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"\\'s\" + 0.005*\"stori\" + 0.005*\"father\" + 0.004*\"power\" + 0.004*\"young\" + 0.004*\"becom\" + 0.004*\"want\" + 0.004*\"time\" + 0.004*\"love\" + 0.004*\"transform\"'),\n",
       " (1,\n",
       "  '0.011*\"\\'s\" + 0.005*\"tri\" + 0.005*\"ship\" + 0.005*\"find\" + 0.004*\"famili\" + 0.004*\"mask\" + 0.004*\"girlfriend\" + 0.004*\"town\" + 0.004*\"world\" + 0.003*\"make\"'),\n",
       " (2,\n",
       "  '0.009*\"\\'s\" + 0.006*\"evil\" + 0.006*\"get\" + 0.006*\"find\" + 0.005*\"treasur\" + 0.005*\"back\" + 0.005*\"year\" + 0.004*\"one\" + 0.004*\"make\" + 0.004*\"world\"'),\n",
       " (3,\n",
       "  '0.018*\"\\'s\" + 0.009*\"one\" + 0.007*\"world\" + 0.007*\"evil\" + 0.006*\"take\" + 0.005*\"life\" + 0.005*\"kill\" + 0.005*\"must\" + 0.005*\"famili\" + 0.004*\"friend\"'),\n",
       " (4,\n",
       "  '0.019*\"\\'s\" + 0.010*\"friend\" + 0.010*\"girl\" + 0.007*\"find\" + 0.007*\"get\" + 0.006*\"shark\" + 0.006*\"tri\" + 0.006*\"live\" + 0.005*\"decid\" + 0.005*\"love\"')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_matrix = lda.show_topics(formatted=False, num_words=20)\n",
    "topics_matrix = np.asarray(topics_matrix,  dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(\"\\'s\", 0.0091705536)', \"('stori', 0.0053786486)\", \"('father', 0.0045391265)\", \"('power', 0.0042730854)\", \"('young', 0.0042711836)\", \"('becom', 0.0041157184)\", \"('want', 0.0040469915)\", \"('time', 0.0039891871)\", \"('love', 0.0038913589)\", \"('transform', 0.0038911509)\", \"('find', 0.0036602519)\", \"('evil', 0.0036095756)\", \"('fall', 0.0035109497)\", \"('friend', 0.0034600263)\", \"('turn', 0.0032869924)\", \"('group', 0.0031278566)\", \"('vampir', 0.0031276133)\", \"('kill', 0.0031270958)\", \"('world', 0.0031262017)\", \"('one', 0.0031257286)\"]\n",
      "\n",
      "['(\"\\'s\", 0.010719056)', \"('tri', 0.0051570642)\", \"('ship', 0.0051210546)\", \"('find', 0.004917311)\", \"('famili', 0.0041483366)\", \"('mask', 0.003956818)\", \"('girlfriend', 0.0038997678)\", \"('town', 0.0038307717)\", \"('world', 0.0036804907)\", \"('make', 0.0034367095)\", \"('get', 0.0034306513)\", \"('want', 0.003411059)\", \"('plan', 0.0034040792)\", \"('one', 0.0033717749)\", \"('fight', 0.0031821656)\", \"('name', 0.0031819777)\", \"('save', 0.0031777148)\", \"('game', 0.0031491271)\", \"('babi', 0.0031444358)\", \"('power', 0.0031238634)\"]\n",
      "\n",
      "['(\"\\'s\", 0.008757228)', \"('evil', 0.0060872049)\", \"('get', 0.0059864763)\", \"('find', 0.0057679513)\", \"('treasur', 0.0053854021)\", \"('back', 0.0049343999)\", \"('year', 0.0048202686)\", \"('one', 0.0043710857)\", \"('make', 0.0042415285)\", \"('world', 0.0041126222)\", \"('man', 0.004104096)\", \"('must', 0.0038462249)\", \"('take', 0.003782755)\", \"('power', 0.0037317693)\", \"('never', 0.0036191486)\", \"('town', 0.0034756279)\", \"('decid', 0.0034345053)\", \"('young', 0.0032645187)\", \"('gold', 0.0032640926)\", \"('secret', 0.0031662113)\"]\n",
      "\n",
      "['(\"\\'s\", 0.018300179)', \"('one', 0.0086380839)\", \"('world', 0.0070903623)\", \"('evil', 0.0070019201)\", \"('take', 0.0059520174)\", \"('life', 0.0053886413)\", \"('kill', 0.0048751151)\", \"('must', 0.0046063093)\", \"('famili', 0.0045228614)\", \"('friend', 0.0042072237)\", \"('find', 0.0040001487)\", \"('young', 0.003980319)\", \"('back', 0.0038710728)\", \"('save', 0.0036753358)\", \"('want', 0.0035561828)\", \"('time', 0.0034771825)\", \"('old', 0.0033654606)\", \"('prison', 0.0033384664)\", \"('year', 0.0032820988)\", \"('turn', 0.0032782117)\"]\n",
      "\n",
      "['(\"\\'s\", 0.019130474)', \"('friend', 0.010279052)\", \"('girl', 0.009769151)\", \"('find', 0.0072977236)\", \"('get', 0.0068050479)\", \"('shark', 0.0060787033)\", \"('tri', 0.0059870686)\", \"('live', 0.0055455808)\", \"('decid', 0.0053243591)\", \"('love', 0.0050827223)\", \"('kill', 0.0047639147)\", \"('one', 0.004693198)\", \"('best', 0.0042393757)\", \"('father', 0.0042033182)\", \"('onli', 0.0041140714)\", \"('meet', 0.0040535498)\", \"('make', 0.0039612451)\", \"('back', 0.003953814)\", \"('young', 0.0038986129)\", \"('tell', 0.0038646804)\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_words = topics_matrix[:,1]\n",
    "for i in topic_words:\n",
    "    print([str(word) for word in i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
